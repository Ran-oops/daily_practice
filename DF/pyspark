docker run --name node0 --hostname node0 --privileged=true -P -p  50071:50070 -p 8088:8088 -p 9870:9870 -p  9864:9864 -p 8080:8080 -p 4040:4040 -p 18080:18080 cent0s7-spark-hadoop-python

docker run --name node1 --hostname node1 --privileged=true  cent0s7-spark-hadoop-python

docker run --name node2 --hostname node2 --privileged=true cent0s7-spark-hadoop-python


docker run -it --name node0 --hostname node0 --privileged=true -P  -p 8088:8088 -p 9870:9870 -p  9864:9864 -p 8080:8080 -p 4040:4040 -p 18080:18080 cent0s7-spark-hadoop-python /bin/bash

docker run -it --name node1 --hostname node1 --privileged=true  cent0s7-spark-hadoop-python /bin/bash
docker run -it --name node2 --hostname node2 --privileged=true cent0s7-spark-hadoop-python /bin/bash

172.17.0.3 node0
172.17.0.4 node1
172.17.0.5 node2

============================
docker network create --subnet=172.11.0.0/20 mynetwork
docker run -it --name node0 --net mynetwork --ip 172.11.0.3 --hostname node0 --privileged=true -P  -p 8088:8088 -p 9870:9870 -p  9864:9864 -p 8080:8080 -p 4040:4040 -p 18080:18080 cent0s7-spark-hadoop-python /bin/bash

docker run -it --name node1 --net mynetwork --ip 172.11.0.4 --hostname node1 --privileged=true  cent0s7-spark-hadoop-python /bin/bash
docker run -it --name node2 --net mynetwork --ip 172.11.0.5 --hostname node2 --privileged=true cent0s7-spark-hadoop-python /bin/bash
172.11.0.3  node0  node0.itcast.cn
172.11.0.4  node1  node1.itcast.cn
172.11.0.5  node2  node2.itcast.cn

172.11.0.3      node0
172.11.0.4      node1
172.11.0.5      node2
172.11.0.6      node3

docker run -d --name node0 --net mynetwork --ip 172.11.0.3 --hostname node0 --privileged -P  -p 8088:8088 -p 9870:9870 -p  9864:9864 -p 8080:8080 -p 4040:4040 -p 18080:18080 cent0s7-spark-hadoop-python init
docker container run -d --name node1 --net mynetwork --ip 172.11.0.4 --hostname node1 --privileged cent0s7-spark-hadoop-python init
docker container run -d --name node2 --net mynetwork --ip 172.11.0.5 --hostname node2 --privileged cent0s7-spark-hadoop-python init




JAVA_HOME=/export/server/jdk1.8.0_211/

HADOOP_CONF_DIR=/export/server/hadoop-3.3.1/etc/hadoop/
YARN_CONF_DIR=/export/server/hadoop-3.3.1/etc/hadoop/

export SPARK_MASTER_HOST=node0.itcast.cn
export SPARK_MASTER_PORT=7077

SPARK_MASTER_WEBUI_PORT=8080
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2g
SPARK_WORKER_PORT=7078
SPARK_WORKER_WEBUI_PORT=8081

SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://node0.itcast.cn:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true"



spark.eventLog.enabled  true
spark.eventLog.dir      hdfs://node0.itcast.cn:8020/sparklog/
spark.eventLog.compress true

export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root




    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node0.itcast.cn:8020</value>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>/export/data/hadoop</value>
    </property>


    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>

    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>



    <!-- 指定secondarynamenode运行位置 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node1.itcast.cn:50090</value>
    </property>









 <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>

    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>








ln -s /export/server/apache-zookeeper-3.9.1-bin /export/server/zookeeper


SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url= node0.itcast.
cn:2181,node1.itcast.cn:2181,node2.itcast.cn:2181 -Dspark.deploy.zookeeper.dir=/spark-ha"






